# Pranav's AI Odyssey: From Data to Intelligence

Welcome to **Pranav's AI Odyssey**, a hands-on journey through the vast fields of AI, Data Science, Machine Learning, Deep Learning, MLOps, NLP, Computer Vision, and Generative AI. This guide includes resources, projects, and gamified milestones to ensure an interactive and efficient learning experience.

## Table of Contents

- [My Learning Approach](#my-learning-approach)
- [How to Consume This?](#how-to-consume-this)
- [Pillar 1: Foundational Knowledge](#pillar-1-foundational-knowledge)
- [Pillar 2: Programming and Tools](#pillar-2-programming-and-tools)
- [Pillar 3: Machine Learning Algorithms and Design](#pillar-3-machine-learning-algorithms-and-design)
- [Pillar 4: MLOps](#pillar-4-mlops)
- [Pillar 5: Deep Learning Fundamentals](#pillar-5-deep-learning-fundamentals)
- [Pillar 6: Deep Learning Frameworks](#pillar-6-deep-learning-frameworks)
- [Pillar 7: Computer Vision](#pillar-7-computer-vision)
- [Pillar 8: NLP](#pillar-8-nlp)
- [Pillar 9: LLMs (Large Language Models)](#pillar-9-llms-large-language-models)
- [Pillar 10: Generative AI](#pillar-10-generative-ai)
- [Pillar 11: Research Paper Analysis and Replication](#pillar-11-research-paper-analysis-and-replication)
- [Pillar 12: Community Engagement and Networking](#pillar-12-community-engagement-and-networking)
- [Pillar 13: Large-scale ETL and Data Engineering](#Pillar-13-Large-scale-ETL-and-Data-Engineering)
- [Pillar 15: Community Engagement and Networking](#PPillar-15-Community-Engagement-and-Networking)
- [Pillar 16: Research and Publication](#pillar-16-research-and-publication)
- [Pillar 14: High-Performance AI Systems and Applications](#pillar-14-high-performance-ai-systems-and-applications)

## My Learning Approach

I believe in a **project-based, hands-on approach** to mastering AI. Each section builds on the previous, with plenty of real-world projects and applications to test the skills I learn. I will revisit core concepts, build strong foundations, and then progressively move to more advanced techniques.

---

## How to Consume This?

- **Beginners**: Start with foundational topics like **Mathematics**, **Programming**, and **Data Science Tools**.
- **Intermediate/Advanced Learners**: You can tweak the curriculum by focusing on advanced topics like **Deep Learning**, **MLOps**, or **LLMs**.
- **Gamified Learning**: Each pillar will have **milestones** and **badges** upon completion. You’ll unlock **bonus content** after completing certain stages of your learning journey.

---

## Pillar 1: Foundational Knowledge

| Topic | Subtopics | Best Resources | Projects/Assignments | Evaluation Criteria |
|-------|-----------|-----------------|----------------------|---------------------|
| **Linear Algebra** | Vectors, matrices, eigenvalues | [MIT OpenCourseWare](https://ocw.mit.edu/courses/mathematics/), [3Blue1Brown](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw) | Implement basic matrix operations | Quiz, problem set |
| **Calculus** | Derivatives, gradients, integrals | [Khan Academy](https://www.khanacademy.org/math/calculus-1), [Coursera](https://www.coursera.org/courses?query=calculus) | Optimization problems, gradient descent implementation | Project, quiz |
| **Probability** | Distributions, Bayes theorem | [Stanford CS109](https://www.youtube.com/playlist?list=PL7XNNfZ_VbsfLerZYxLOm_Vy7T9cVA7w3), [Khan Academy](https://www.khanacademy.org/math/statistics-probability) | Probabilistic modeling project | Quiz, project |
| **Statistics** | Hypothesis testing, regression | [StatQuest](https://www.youtube.com/user/joshstarmer), [EdX](https://www.edx.org/learn/statistics) | Data analysis project | Project review |
| **Information Theory** | Entropy, mutual information | "Elements of Information Theory" (book) | Implement compression algorithm | Project |

---

## Pillar 2: Programming and Tools

| Skill | Subtopics | Best Resources | Projects/Assignments | Evaluation Criteria |
|-------|-----------|-----------------|----------------------|---------------------|
| **Python** | OOP, functional programming | [Real Python](https://realpython.com), [Python.org](https://docs.python.org/3/tutorial/) | Build a data processing pipeline | Code review |
| **Git** | Version control, branching | [GitHub Learning Lab](https://lab.github.com/), [Pro Git](https://git-scm.com/book/en/v2) | Contribute to an open-source project | Code review |
| **Linux** | Shell scripting | [Linux Journey](https://linuxjourney.com), [The Linux Command Line](http://linuxcommand.org/tlcl.php) | Implement a process scheduler | Practical test |
| **Flask** | Web application | [Flask Documentation](https://flask.palletsprojects.com/en/2.0.x/) | Build a simple ML API | Code review |
| **Streamlit** | Dashboard building | [Streamlit Documentation](https://docs.streamlit.io) | Build an EDA dashboard | Code review |
| **SQL** | Queries, Joins | [SQLZoo](https://sqlzoo.net/), [LeetCode SQL](https://leetcode.com/problemset/database/) | Build a database for an ML application | Project review |

---

## Pillar 3: Machine Learning Algorithms and Design

| Algorithm | Subtopics | Best Resources | Projects/Assignments | Evaluation Criteria |
|-----------|-----------|-----------------|----------------------|---------------------|
| **Linear Regression** | Simple Linear, Multiple Linear | [Hands-On Machine Learning](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/), [Scikit-learn](https://scikit-learn.org/stable/) | Predict house prices | R2 score, MSE |
| **Polynomial Regression** | Polynomial features, overfitting | [Scikit-learn](https://scikit-learn.org/stable/) | Predict nonlinear relationships | R2 score, MSE |
| **Logistic Regression** | Binary classification, Sigmoid | [Scikit-learn](https://scikit-learn.org/stable/) | Classify emails as spam/non-spam | Accuracy, F1 score |
| **Support Vector Machines (SVM)** | Linear, non-linear kernels | [Hands-On Machine Learning](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) | Classify handwritten digits | Accuracy, precision, recall |
| **Decision Trees** | Overfitting, pruning, Gini index | [Scikit-learn](https://scikit-learn.org/stable/) | Build a classification tree for customer churn | Accuracy, Gini index |
| **Random Forest** | Ensemble learning, bagging | [Scikit-learn](https://scikit-learn.org/stable/) | Predict heart disease | Accuracy, ROC AUC |
| **K-Nearest Neighbors (KNN)** | Distance metrics, classification | [Scikit-learn](https://scikit-learn.org/stable/) | Classify species of flowers | Accuracy |
| **Naive Bayes** | Conditional probability, Bayes' theorem | [StatQuest](https://www.youtube.com/user/joshstarmer) | Classify movie reviews as positive/negative | Accuracy, confusion matrix |
| **Gradient Boosting (XGBoost, LightGBM)** | Boosting, overfitting, learning rate | [XGBoost Docs](https://xgboost.readthedocs.io/en/latest/), [LightGBM](https://lightgbm.readthedocs.io/en/latest/) | Predict customer churn | Accuracy, AUC |
| **K-Means Clustering** | Centroids, cluster optimization | [KMeans Tutorial](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) | Cluster customers based on purchasing behavior | Silhouette score |
| **DBSCAN** | Density-based clustering | [Scikit-learn](https://scikit-learn.org/stable/) | Identify fraudsters in transactions | Silhouette score |
| **Principal Component Analysis (PCA)** | Dimensionality reduction | [Scikit-learn](https://scikit-learn.org/stable/) | Visualize high-dimensional data | Explained variance ratio |

---

## Pillar 4: MLOps

| Topic | Subtopics | Best Resources | Projects/Assignments | Evaluation Criteria |
|-------|-----------|-----------------|----------------------|---------------------|
| **Model Deployment** | Flask, Docker, Kubernetes | [Docker Docs](https://docs.docker.com/), [Kubernetes Docs](https://kubernetes.io/docs/) | Deploy a model on AWS/GCP | Deployment success |
| **CI/CD Pipelines** | GitHub Actions, Jenkins | [GitHub Actions](https://docs.github.com/en/actions), [Jenkins Docs](https://www.jenkins.io/doc/) | Set up a CI/CD pipeline for model deployment | Pipeline efficiency |
| **Version Control for ML** | DVC, Git | [DVC Docs](https://dvc.org/), [Git](https://git-scm.com/book/en/v2) | Use DVC to track models and datasets | Version tracking success |
| **Monitoring and Logging** | Model drift, MLflow | [MLflow Docs](https://www.mlflow.org/), [Kedro](https://kedro.readthedocs.io/en/stable/) | Create a model monitoring dashboard | Model drift detection |

---

## Pillar 5: Deep Learning Fundamentals

| Topic | Subtopics | Best Resources | Projects/Assignments | Evaluation Criteria |
|-------|-----------|-----------------|----------------------|---------------------|
| **Artificial Neural Networks (ANN)** | Feedforward, backpropagation | [Deep Learning Book](https://www.deeplearningbook.org/), [CS231n](http://cs231n.stanford.edu/) | Build a neural network for classification | Accuracy, loss |
| **Convolutional Neural Networks (CNN)** | Filters, pooling, CNN architectures | [CS231n](http://cs231n.stanford.edu/), [Keras Docs](https://keras.io/) | Classify images from CIFAR-10 | Accuracy, confusion matrix |
| **Recurrent Neural Networks (RNN)** | LSTM, GRU, sequence modeling | [D2L.ai](https://d2l.ai/), [Fast.ai](https://www.fast.ai/) | Sentiment analysis on IMDB reviews | Accuracy, F1 score |
| **Transformers** | Attention mechanism, BERT, GPT | [Attention is All You Need](https://arxiv.org/abs/1706.03762), [Hugging Face](https://huggingface.co/) | Implement a Transformer for machine translation | BLEU score |
| **Optimization Techniques** | SGD, Adam, learning rate schedules | [Sebastian Ruder](https://rpradeepmenon.medium.com/), [Deep Learning Book](https://www.deeplearningbook.org/) | Compare optimizers on a benchmark task | Learning curves |
| **Regularization** | Dropout, batch normalization | [Deep Learning Book](https://www.deeplearningbook.org/), [CS231n](http://cs231n.stanford.edu/) | Implement a regularized neural network | Loss, validation accuracy |

---

## Pillar 6: Deep Learning Frameworks 

| Framework | Subtopics | Best Resources | Projects/Assignments | Evaluation Criteria |
|-----------|-----------|-----------------|----------------------|---------------------|
| **PyTorch** | Tensors, Autograd, CNN, RNN | [PyTorch Docs](https://pytorch.org/), [Fast.ai](https://www.fast.ai/) | Implement a neural network | Model accuracy |
| **TensorFlow** | Keras API, TensorFlow Hub | [TensorFlow Docs](https://www.tensorflow.org/), [Keras](https://keras.io/) | Build a CNN for image classification | Model accuracy |
| **Keras** | Deep learning models | [Keras Documentation](https://keras.io/) | Build a sentiment analysis model | Accuracy, F1 score |

---

## Pillar 7: Computer Vision

| Topic | Subtopics | Best Resources | Projects/Assignments | Evaluation Criteria |
|-------|-----------|-----------------|----------------------|---------------------|
| **Image Preprocessing** | Resizing, normalization, augmentation | [OpenCV Docs](https://opencv.org/), [PyImageSearch](https://pyimagesearch.com/) | Apply transformations to a dataset | Image quality |
| **Object Detection** | YOLO, SSD, Faster R-CNN | [TensorFlow Object Detection](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/), [PyTorch Tutorials](https://pytorch.org/tutorials/) | Detect objects in a real-world image | mAP score |
| **Image Classification** | CNN architectures, Transfer learning | [Fast.ai](https://www.fast.ai/), [CS231n](http://cs231n.stanford.edu/) | Build a classifier for Cats vs Dogs dataset | Accuracy, loss |
| **Segmentation** | U-Net, Mask R-CNN | [PyImageSearch](https://pyimagesearch.com/), [Keras Docs](https://keras.io/) | Implement a semantic segmentation model | IoU score |
| **Generative Adversarial Networks (GANs)** | Generator, Discriminator, Training stability | [Goodfellow GAN Paper](https://arxiv.org/abs/1406.2661), [Hugging Face](https://huggingface.co/) | Generate synthetic faces with a GAN | Image quality, FID score |

---

## Pillar 8: NLP

| Topic | Subtopics | Best Resources | Projects/Assignments | Evaluation Criteria |
|-------|-----------|-----------------|----------------------|---------------------|
| **Word Embeddings** | Word2Vec, GloVe | [Stanford NLP](https://web.stanford.edu/class/cs224n/), [Gensim](https://radimrehurek.com/gensim/) | Implement word2vec for text corpus | Cosine similarity |
| **Neural Networks for NLP** | RNN, LSTM, GRU | [D2L.ai](https://d2l.ai/), [Fast.ai](https://www.fast.ai/) | Build a sentiment analysis model | Accuracy, F1 score |
| **Transformers for NLP** | BERT, GPT-2, T5 | [Hugging Face Course](https://huggingface.co/course), [OpenAI GPT Papers](https://openai.com/research/) | Fine-tune GPT-2 for text generation | BLEU score |
| **Text Classification** | Text preprocessing, Tokenization | [Stanford NLP](https://web.stanford.edu/class/cs224n/), [Fast.ai](https://www.fast.ai/) | Implement a spam classifier | Precision, Recall |
| **Named Entity Recognition (NER)** | Sequence labeling, CRF | [SpaCy](https://spacy.io/), [Stanford NLP](https://stanfordnlp.github.io/CoreNLP/) | Build a NER system | F1 score |
| **Text Generation** | Markov Chains, LSTMs, Transformers | [Hugging Face](https://huggingface.co/), [Fast.ai](https://www.fast.ai/) | Implement a chatbot using GPT-2 | Text quality, coherence |

---

# Pillar 9: Large Language Models (LLMs)

## Large Language Models (LLMs)

| ✅ | Subtopic                         | Description                              | Best Resources                                                                                       | Projects/Assignments                          | Evaluation Criteria                      |
|----|----------------------------------|------------------------------------------|------------------------------------------------------------------------------------------------------|-----------------------------------------------|------------------------------------------|
| ⬜ | **Overview of LLMs**              | Evolution of language models, basics of GPT, BERT, T5 | [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/), [Hugging Face Blog](https://huggingface.co/blog/) | Write a summary of the evolution of LLMs     | Depth of understanding, clarity          |
| ⬜ | **Tokenization**                 | Subword tokenization (BPE, WordPiece), character-level tokenization | [Hugging Face Tokenizers](https://huggingface.co/docs/tokenizers), [Stanford NLP](https://nlp.stanford.edu/) | Implement custom tokenization for a dataset  | Efficiency, correctness                  |
| ⬜ | **Transformer Architecture**     | Attention mechanism, self-attention, encoder-decoder architecture | [Attention Is All You Need](https://arxiv.org/abs/1706.03762), [CS231n](http://cs231n.stanford.edu/) | Implement self-attention in PyTorch          | Code clarity, understanding              |
| ⬜ | **BERT (Bidirectional Encoder)** | Masked language modeling, next-sentence prediction | [BERT Paper](https://arxiv.org/abs/1810.04805), [Hugging Face Course](https://huggingface.co/course) | Fine-tune BERT on a text classification task | Model accuracy, F1 score                 |
| ⬜ | **GPT (Generative Pre-trained Transformer)** | Causal language modeling, autoregressive text generation | [GPT-2 Paper](https://arxiv.org/abs/1901.04587), [Hugging Face Transformers](https://huggingface.co/transformers/) | Fine-tune GPT-2 for text generation          | Perplexity, BLEU score                   |
| ⬜ | **T5 (Text-to-Text Transfer Transformer)** | Text-to-text framework for summarization, translation, classification | [T5 Paper](https://arxiv.org/abs/1910.10683), [Hugging Face T5](https://huggingface.co/docs/transformers/model_doc/t5) | Implement text summarization with T5         | ROUGE score, readability                 |
| ⬜ | **Pre-training and Fine-tuning** | Techniques for pre-training and domain-specific fine-tuning of LLMs | [Hugging Face Fine-tuning Guide](https://huggingface.co/course), [OpenAI Fine-tuning](https://platform.openai.com/docs/guides/fine-tuning) | Pre-train a small transformer on a custom dataset | Training efficiency, validation loss     |
| ⬜ | **Prompt Engineering**           | Techniques for zero-shot, few-shot learning using prompts | [OpenAI Prompt Engineering](https://platform.openai.com/docs/guides/completions/prompt-design), [Hugging Face](https://huggingface.co/) | Design prompts for text classification       | Model performance, prompt creativity     |
| ⬜ | **Evaluation of LLMs**           | Metrics like BLEU, ROUGE, perplexity, and human evaluations | [Hugging Face Metrics](https://huggingface.co/docs/evaluate/index), [SQuAD Dataset](https://rajpurkar.github.io/SQuAD-explorer/) | Evaluate a fine-tuned GPT model for QA       | Metric scores, human evaluation          |
| ⬜ | **Ethics and Bias in LLMs**      | Addressing biases, fairness, and hallucinations in LLM outputs | [TruthfulQA Paper](https://arxiv.org/abs/2109.07958), [OpenAI Blog](https://openai.com/blog/)         | Detect and mitigate bias in GPT-3 outputs    | Reduction in biased responses            |
| ⬜ | **Future Directions**            | Research areas like retrieval-augmented generation (RAG), LoRA | [RAG Paper](https://arxiv.org/abs/2005.11401), [LoRA Paper](https://arxiv.org/abs/2106.09685)        | Implement RAG for knowledge-intensive tasks | Retrieval accuracy, response quality     |


## Pillar 10: Generative AI

| Topic | Subtopics | Best Resources | Projects/Assignments | Evaluation Criteria |
|-------|-----------|-----------------|----------------------|---------------------|
| **Diffusion Models** | Denoising, reverse diffusion | [OpenAI Research](https://openai.com/research), [Hugging Face](https://huggingface.co/) | Implement a diffusion model for image generation | Image quality |
| **Generative Adversarial Networks (GANs)** | Conditional GANs, DCGANs, StyleGAN | [Goodfellow GAN Paper](https://arxiv.org/abs/1406.2661), [Hugging Face](https://huggingface.co/) | Build a GAN for generating artwork | FID score, Image diversity |
| **Text-to-Image Generation** | DALL-E, CLIP, VQ-VAE | [OpenAI DALL-E](https://openai.com/blog/dall-e), [Hugging Face](https://huggingface.co/) | Create a text-to-image generator | Image quality, relevance |
| **Multimodal Models** | CLIP, Vision Transformers | [OpenAI CLIP](https://openai.com/research/clip), [Hugging Face](https://huggingface.co/) | Implement a multimodal classifier | Accuracy, performance |

---

## Pillar 11: Research Paper Analysis and Replication

| Activity | Approach | Resources | Deliverables | Evaluation Criteria |
|----------|----------|-----------|--------------|---------------------|
| Paper Reading | Weekly reading of latest papers | arXiv, Papers With Code | Paper summaries, critiques | Quality of analysis, subjective evaluation |
| Research Replication | Reproduce results of significant papers | Original papers, open-source implementations | Replicated experiments, results comparison | Accuracy of replication, subjective evaluation |
| Trend Analysis | Identify and analyze research trends | NeurIPS, ICML, ACL proceedings | Trend reports, blog posts | Insight quality, subjective evaluation |
| Paper Presentation | Present papers to peers or online | Conference recordings | Video explanations, slide decks | Presentation quality, subjective evaluation |
| Extension Projects | Extend or combine ideas from papers | Related work sections of papers | Novel experiments, blog posts | Originality, subjective evaluation |
| Visualization Projects | Develop interactive visualizations for ML concepts | D3.js, Plotly | Create an interactive visualization of attention between tokens in a language model | Visualization quality, insight provided, subjective evaluation |


## Pillar 12: High-Performance AI Systems and Applications

| Topic | Subtopics | Learning Resources | Projects/Assignments | Evaluation Criteria |
|-------|-----------|---------------------|----------------------|---------------------|
| Model Optimization | Quantization, pruning, distillation | TensorFlow Model Optimization Toolkit | Optimize a large model for edge devices | Performance benchmarks, subjective evaluation |
| Efficient Architectures | MobileNet, EfficientNet, BERT-tiny | Papers, GitHub implementations | Implement and benchmark efficient models | Comparative analysis, subjective evaluation |
| Hardware Acceleration | GPU programming, TPU utilization | NVIDIA Deep Learning Institute | Optimize a model for specific hardware | Performance improvement, subjective evaluation |
| Scalable AI Systems | Microservices, containerization, orchestration | Kubernetes documentation, Docker tutorials | Design a scalable AI service architecture | Architecture review, subjective evaluation |  
| ML Ops | CI/CD for ML, model monitoring | Google MLOps guides | Set up an MLOps pipeline | Pipeline effectiveness, subjective evaluation |


## Pillar 13: Large-scale ETL and Data Engineering

| Topic | Subtopics | Learning Resources | Projects/Assignments | Evaluation Criteria |
|-------|-----------|---------------------|----------------------|---------------------|
| ETL Fundamentals | Data extraction, transformation, loading | Udacity Data Engineering course | Design and implement an ETL pipeline | Pipeline efficiency, subjective evaluation |
| Distributed Data Processing | Apache Spark, Hadoop | Spark documentation, Coursera courses | Process and analyze a large dataset using Spark | Processing speed, scalability, subjective evaluation |
| Data Warehousing | Star schema, OLAP | Kimball Group resources | Design a data warehouse for ML experiments | Design review, subjective evaluation |
| Streaming Data | Kafka, Flink | Confluent Kafka tutorials | Implement a real-time data processing pipeline | System performance, subjective evaluation |

## Pillar 14: Ethical AI and Responsible Development

| Topic | Subtopics | Learning Resources | Projects/Assignments | Evaluation Criteria |
|-------|-----------|---------------------|----------------------|---------------------|
| AI Ethics | Fairness, accountability, transparency | MIT Moral Machine, ethicalML.org | Develop an AI ethics framework | Framework review, subjective evaluation |
| Bias in AI | Dataset bias, algorithmic bias | "Gender Shades" paper, AI Fairness 360 | Audit a model for bias | Report review, subjective evaluation |
| Privacy in ML | Differential privacy, federated learning | OpenMined tutorials | Implement privacy-preserving ML | Project review, subjective evaluation |
| Explainable AI | LIME, SHAP, counterfactual explanations | Interpretable Machine Learning book | Develop model explanations | Project review, subjective evaluation |
| AI Governance | Regulations, guidelines, best practices | EU AI Act, IEEE Ethically Aligned Design | Propose AI governance structure | Proposal review, subjective evaluation |

## Pillar 15: Community Engagement and Networking

| Activity | Approach | Resources | Deliverables | Evaluation Criteria |
|----------|----------|-----------|--------------|---------------------|
| Conference Participation | Attend or present at AI conferences | Conference websites, Call for Papers | Presentation slides, trip reports | Engagement quality, subjective evaluation |
| Open Source Contribution | Contribute to AI/ML open source projects | GitHub, PyPi | Code contributions, pull requests | Contribution impact, subjective evaluation |
| AI Community Building | Organize meetups, study groups | Meetup.com, local tech communities | Event reports, community growth metrics | Community impact, subjective evaluation |
| Online Presence | Blog writing, social media engagement | Medium, Twitter, LinkedIn | Blog posts, thread discussions | Reach and engagement, subjective evaluation |
| Collaborative Projects | Partner with other researchers/engineers | Academic collaborations, hackathons | Joint projects, co-authored papers | Collaboration quality, subjective evaluation |

## Pillar 16: Research and Publication

| Activity | Approach | Resources | Deliverables | Evaluation Criteria |
|----------|----------|-----------|--------------|---------------------|
| Research Ideation | Brainstorming, literature review | arXiv, Google Scholar | Research proposals | Originality, feasibility, subjective evaluation |
| Experiment Design | Hypothesis formulation, methodology planning | Research design books, mentorship | Experimental protocols | Rigor, subjective evaluation |
| Data Collection and Analysis | Data gathering, statistical analysis | Kaggle datasets, statistical tools | Datasets, analysis reports | Data quality, analysis depth, subjective evaluation |
| Paper Writing | Scientific writing, LaTeX | Overleaf, writing workshops | Draft papers | Writing quality, subjective evaluation |
| Peer Review | Understand and participate in peer review process | PubliONS, Elsevier Researcher Academy | Review reports | Review quality, subjective evaluation |
| Publication and Presentation | Submit to journals/conferences, create posters | Journal guidelines, conference deadlines | Published papers, conference posters | Impact factor, presentation quality, subjective evaluation |
